# -*- coding: utf-8 -*-
"""ANN with Diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wx15dh3UTW2v_e9fwfrKyt5BK8SWKgnD

# DIABETES CLASSIFICATION USING NEURAL NETWORKS

## PROBLEM STATEMENT:

---
- This dataset is used to predict whether or not a patient has diabetes, based on given features/diagnostic measurements.
- Only female patients are considered with at least 21 years old of Pima Indian heritage.

- Inputs:
    - Pregnancies: Number of times pregnant
    - GlucosePlasma: glucose concentration a 2 hours in an oral glucose tolerance test
    - BloodPressure: Diastolic blood pressure (mm Hg)
    - Skin: ThicknessTriceps skin fold thickness (mm)
    - Insulin: 2-Hour serum insulin (mu U/ml)
    - BMI: Body mass index (weight in kg/(height in m)^2)
    - DiabetesPedigreeFunction: Diabetes pedigree function
    - Age: Age (years)
- Outputs:
    - Diabetes or no diabetes (0 or 1)
    
    
- Acknowledgements
Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.

## STEP 0: IMPORT LIBRARIES
"""

!pip install scikit-optimize

!pip install keras-tuner

import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

from keras.models import Sequential
from keras.layers import Dense

from kerastuner.tuners import BayesianOptimization  # Import the BayesianOptimization tuner
from imblearn.over_sampling import SMOTE #to balance the dataset
import tensorflow as tf
import pickle

"""## STEP 1: IMPORTING THE DATASET"""

# Load the diabetes dataset
diabetes = pd.read_csv('diabetes.csv')
diabetes.head()

diabetes.tail()

diabetes.info()

"""## STEP 2: VISUALIZE THE DATASET

### 2.1: Representing the target variable:
"""

sns.countplot(x = 'Outcome', data = diabetes)

"""Comment:     
We can see that this dataset is not balanced. Therefore we will have to perform some sort of intervention in order to balance it out when dealing with the training set. This will be done in part 4.2.

### 2.2 Data Distribution and relation to Target:         
I will look into the boxplots of each features, this way I can see how the feature values are distributed with respect to each class. This can be helpful to identify if any features have different median values for different classes, which is useful for classification.
"""

# Create a figure to hold the subplots
plt.figure(figsize=(15, 10))

# Loop through each feature and create a boxplot
for i, feature in enumerate(diabetes.columns[:-1]):  # Exclude the 'Outcome' column which is not a feature but the target
    plt.subplot(3, 3, i + 1)  # Arrange the subplots in a 3x3 grid
    sns.boxplot(x='Outcome', y=feature, data=diabetes, palette="coolwarm")
    plt.title(feature)

plt.tight_layout()
plt.show()

"""Looking at this representation, Glucose seems to be one of the main feature to differentiate best who has diabetes who has not. Age seems to also be a relevant vraiable. On the other hand skin thinkness seems to be pretty irrelevant."""

sns.pairplot(diabetes, hue = 'Outcome', vars = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])

"""This pairplot confirms the previous observation. Glucose seems to be te most relevant variable to explain diabetes.

### 2.3 Correlation Matrix:
"""

# Set up the matplotlib figure
plt.figure(figsize=(8, 5))
# Draw the heatmap
sns.heatmap(diabetes.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix of Diabetes Features')
plt.show()

"""As we expected, from this correlation analysis we observe that the features *BloodPressure* and *SkinThinkcness* have a very low correlation with our target variable. This indicates that they do not have a big explainabilty when it comes to detecting diabetes. Thererefore, I will remove them from our database.

We do not observe any high correlation between features that would indicate multicolinearity, so we can keep all the other variables.

## STEP 3: DATA CLEANING

Removing the features that were decided not to be relevant in identifying diabetes cases.
"""

# Drop the 'BloodPressure' and 'SkinThickness' columns from the dataframe
diabetes_new = diabetes.drop(['BloodPressure', 'SkinThickness'], axis=1)

diabetes_new.info()

"""## STEP 4: Train & Test split and Preprocessing:

### 4.1: Training and Testing Datasets Creation:
"""

diabetes_new.head()

# Split the dataset into features (X) and target (y)
X = diabetes_new.drop('Outcome',axis=1)
y = diabetes_new['Outcome']

X.head()

y.head()

# Splitting the dataset into the Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

"""### 4.2: Balancing the training set:   
In order to balance the training dataset I will use the technique we saw in class: *Synthetic Minority Over-sampling Technique* (SMOTE). It is an approach to handle imbalanced datasets by creating synthetic samples from the minority class.
"""

# Handling Data Imbalance using SMOTE
sm = SMOTE(random_state=2)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# Visualize the balanced data
sns.countplot(x=y_train_res, label="Count")
plt.title('Balanced Classes')
plt.show()

"""### 4.3: Scaling the dataset:
Scaling is a crucial step for Neural Networks.They benefit from feature scaling, such as normalization or standardization, to ensure all input features contribute equally to the model's prediction.
"""

# Feature Scaling is a must with Neural Networks
sc = StandardScaler()
X_train_res_sc = sc.fit_transform(X_train_res)
X_test_sc = sc.transform(X_test)

"""## STEP 5: CREATE AND TRAIN THE NEURAL NETWORK MODEL

Initially, I created a model with 4 layers, aiming to capture the patterns in the dataset that could help in predicting diabetes. The model structure was as follow:

```
# Define a function to build the Keras model
def build_model(hp):
    model = Sequential()
    model.add(Dense(units=hp.Int('hidden_units', min_value=100, max_value=500, step=50), activation=hp.Choice('activation', values=['relu', 'tanh'])))
    model.add(Dense(units=hp.Int('hidden_units', min_value=100, max_value=500, step=50), activation=hp.Choice('activation', values=['relu', 'tanh'])))
    model.add(Dense(units=1, activation='sigmoid'))
    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'sgd']), loss='binary_crossentropy', metrics=['accuracy'])
    return model
```
However, this configuration led to overfitting, evident from the loss metrics during training; the training loss continued to decrease while the validation loss did not, indicating that the model was learning patterns specific to the training data that did not generalize well to unseen data.

To address this issue, I introduced two key regularization techniques:
- **L2 Regularization:** This technique penalizes the squared magnitude of the coefficients, which helps to keep the weights small and, in turn, the model simpler. The hypothesis is that a simpler model would generalize better. L2 regularization was added to each dense layer.
- **Dropout:** This technique randomly sets input units to 0 with a frequency of rate at each step during training, which helps to prevent overfitting by making the neural network's nodes less sensitive to the weights of other nodes. It effectively trains a subset of the nodes at each step, resulting in a more robust network that generalizes better.

By incorporating these changes, I expect the model to maintain a balance between bias and variance, better capturing the underlying trends without adhering too closely to the training data specifics. The hyperparameters for the regularization and dropout rates are to be determined via hyperparameter tuning, ensuring that we find the most effective values that contribute to a well-generalized model.
"""

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.regularizers import l2

# Define a function to build the Keras model
def build_model(hp):
    model = Sequential()
    # First dense layer with L2 regularization
    model.add(Dense(
        units=hp.Int('first_layer_units', min_value=100, max_value=500, step=50),
        activation=hp.Choice('first_layer_activation', values=['relu', 'tanh']),
        kernel_regularizer=l2(hp.Float('first_layer_l2', min_value=0.0, max_value=0.01, step=0.001))
    ))
    # Dropout layer after the first dense layer
    model.add(Dropout(hp.Float('first_layer_dropout', min_value=0.0, max_value=0.5, step=0.1)))
    # Second dense layer with L2 regularization
    model.add(Dense(
        units=hp.Int('second_layer_units', min_value=100, max_value=500, step=50),
        activation=hp.Choice('second_layer_activation', values=['relu', 'tanh']),
        kernel_regularizer=l2(hp.Float('second_layer_l2', min_value=0.0, max_value=0.01, step=0.001))
    ))
    # Dropout layer after the second dense layer
    model.add(Dropout(hp.Float('second_layer_dropout', min_value=0.0, max_value=0.5, step=0.1)))
    # Output layer
    model.add(Dense(units=1, activation='sigmoid'))
    # Compile the model
    model.compile(
        optimizer=hp.Choice('optimizer', values=['adam', 'sgd']),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

# Create the Bayesian Optimization tuner
opt = BayesianOptimization(
    build_model,
    objective='val_accuracy',
    max_trials=32,
    num_initial_points=5,
    directory='my_dir',
    project_name='diabetes_tuner'
)

# Search for the best hyperparameters
opt.search(X_train_res_sc, y_train_res, validation_data=(X_test_sc, y_test), epochs=150)

# Get the best hyperparameters
best_hyperparameters = opt.get_best_hyperparameters()[0]
print("Best Hyperparameters:", best_hyperparameters)

# Build the model with the best hyperparameters
best_model = build_model(best_hyperparameters)

# Compile and fit the model
best_model.compile(optimizer=best_hyperparameters['optimizer'],
                   loss='binary_crossentropy',
                   metrics=['accuracy'])

epochs_hist=best_model.fit(
    X_train_res_sc,
    y_train_res,
    validation_data=(X_test_sc, y_test),
    epochs=30)

best_model.summary()

"""## STEP 6: Make Predictions:"""

# Make predictions on the test set
y_pred = best_model.predict(X_test_sc)
y_pred = (y_pred > 0.5)

"""## STEP 7: Evaluate the model:

### 7.1: Analysing Overfitting:
"""

epochs_hist.history.keys()

# Plot training & validation loss values
plt.plot(epochs_hist.history['loss'], 'r', label='Training loss')
plt.plot(epochs_hist.history['val_loss'], 'b', label='Validation loss')
plt.title('Model Loss Progress During Training')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""Unfortunately we observe a high overfitting as the validation loss is increasing as the training loss decreases. This could be due to the fact that maybe having neural networks is a too advanced model for the diabetes dataset and we should try an simpler one.  



From this graphic we can draw several conclusions on the model:
- **Fluctuating Validation Loss:** The validation loss shows significant fluctuations throughout the training epochs, which typically suggests a model struggling to generalize. These fluctuations are a sign that while the model is learning from the training data, its predictions on unseen data are not consistently improving.
- **Trend of Overfitting:** We can observe that the validation loss is mainely above the training loss. Also, the presence of a generally decreasing training loss alongside a volatile validation loss, especially with peaks at epochs 6, 7, and 12, indicates a trend towards overfitting. Despite the implemented regularization techniques, the model appears to be fitting too closely to the training data at various points.
- **Potential Early Stopping Points:** There are epochs where the validation loss reaches a minimum before increasing again (such as epochs 10, 13, 17, 22, and 29). These points are potential candidates for early stopping, which would prevent the model from continuing to learn noise and non-generalizable patterns.
- **Regularization Effectiveness:** The regularization methods added to reduce overfitting seem to have an effect, as shown by the eventual decrease in validation loss at several points. However, the effect is not stable, suggesting that further tuning of regularization parameters or the training process is needed.
- **Learning Rate Optimization:** The oscillations in validation loss suggest that the learning rate may benefit from fine-tuning. Using techniques such as learning rate schedules or adaptive learning rate methods could lead to more stable and consistent improvements.
- **Model Capacity and Complexity:** Given the pattern of losses, it may be worthwhile to explore adjustments in the model's capacity. This could involve altering the number of layers, the number of neurons in each layer, or even the model's architecture to strike a better balance between learning and generalization.

### 7.2: Training Set Analysis:
"""

#Training set performance:
y_train_pred = best_model.predict(X_train_res_sc)
y_train_pred = (y_train_pred > 0.5)
cm = confusion_matrix(y_train_res, y_train_pred)
sns.heatmap(cm, annot=True)

"""Training Set Performance (Code 1 & Result 1)
The confusion matrix for the training set indicates the following:
- True negatives (TN) = 290 (predicted no diabetes, actually no diabetes)
- False negatives (FN) = 82 (predicted no diabetes, actually diabetes)
- False positives (FP) = 110 (predicted diabetes, actually no diabetes)
- True positives (TP) = 320 (predicted diabetes, actually diabetes)

Interpretation:     
This means that the model has a higher number of true positives and true negatives, which suggests it can identify both classes reasonably well.
However, there are still a significant number of false positives and false negatives, indicating room for improvement.
"""

print(classification_report(y_train_pred, y_train_res))

"""The classification report for the training data provides the following metrics:
- Precision (True Positives / (True Positives + False Positives)) measures the accuracy of positive predictions.
  - Precision for class False: 0.73
  - Precision for class True: 0.80
- Recall (True Positives / (True Positives + False Negatives)) measures the ability of the classifier to find all the positive samples.
  - Recall for class False: 0.78
  - Recall for class True: 0.75
- F1-score is the harmonic mean of precision and recall and gives a combined idea about the performance.
  - F1-score for class False: 0.76
  - F1-score for class True: 0.77
- Support is the number of actual occurrences of the class in the specified dataset.
  - Support for class False: 375
  - Support for class True: 427
- Accuracy (Overall, how often is the classifier correct?): 0.76

Interpretation:     
The classification report shows that the model performs slightly better in predicting the positive class than the negative, which could be important in a medical diagnosis context where missing a positive case could be more dangerous than a false alarm.

### 7.3: Testing Set Analysis:
"""

# Evaluate the model on the test set
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True)
print(classification_report(y_test, y_pred))

"""From the confusion matrix and classification report for the test set we can say that:
- The model's precision is lower for predicting diabetes (1) than not (0). This suggests that when the model predicts diabetes, it is correct 58% of the time, but when it predicts no diabetes, it is correct 84% of the time.
- The recall is higher for the diabetes class than for the no diabetes class, which means the model is better at identifying all the positive diabetes cases than the negative ones.
- The f1-score for both classes is relatively balanced, indicating that the model does not excessively favor one class over the other.
- The overall accuracy on the test set is 0.71, which is lower than the training set accuracy of 0.76. This drop in performance suggests that the model may be overfitting to the training data.

### **CONCLUSION:**       
The model seems to perform reasonably well on the training set but shows a performance drop on the test set, indicating potential overfitting despite the added regularization and dropout layers. There may be a need to further tune the regularization parameters, gather more data, or try different architectures to improve generalization. It's also crucial to ensure that the test set is large and diverse enough to be representative of the problem space.

## STEP 8: Save the model:
"""

# Assuming 'best_model' is your Keras model
model = best_model

# Save the model in HDF5 format
model_filename = 'diabetes_classifier.h5'
model.save(model_filename)

# Save the scaler to a file
with open('scaler.pkl', 'wb') as scaler_file:
    pickle.dump(sc, scaler_file)